{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Data Wrangling\n",
    "\n",
    "*Learn to prepare data for visualization and analytics.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "This tutorial provides step-by-step training divided into numbered sections. The sections often contain embeded exectable code for demonstration.  This tutorial is accompanied by a practice notebook: [L06-Data_Wrangling-Practice.ipynb](./L06-Data_Wrangling-Practice.ipynb). \n",
    "\n",
    "Throughout this tutorial sections labeled as \"Tasks\" are interspersed and indicated with the icon: ![Task](http://icons.iconarchive.com/icons/sbstnblnd/plateau/16/Apps-gnome-info-icon.png). You should follow the instructions provided in these sections by performing them in the practice notebook.  When the tutorial is completed you can turn in the final practice notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The purpose of this assignment is to build on Tidy data cleaning by using Python tools to \"massage\" or \"wrangle\" data into formats that are most useful for visualization and analytics.\n",
    "\n",
    "**What is data wrangling?**\n",
    "\n",
    "> Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. \n",
    "\n",
    "- [Data Wangling](https://en.wikipedia.org/wiki/Data_wrangling) *Wikipedia*\n",
    "\n",
    "Previously, we learned about Tidy rules for reformatting data.  Transforming data into a Tidy dataset is data wrangling.  We have also learned to how to correct data types, remove missing values and duplicates.  This lessons is, therefore, an opportunity to bring everything together.  Some of the material will be a review, but should help reinforce the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Getting Started\n",
    "As before, we import any needed packages at the top of our notebook. Let's import Numpy and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Exploration\n",
    "The first step in any data analytics task is import and exploration of data.  At this point, we have learned all of the steps we need to identify the data columns, their data types, recognize where we have missing values and recognize categorical and numeric variables in the data.   \n",
    "\n",
    "For this tutorial we will use a dataset named \"Abolone\" from the [University of California Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Abalone). The datafile is named `abalone.data` and is available in the data directory that accompanies this notebook.  The data has 10 \"attributes\" or variables. The following table describes these 10 variables, their types, and additional details.\n",
    "\n",
    "<table>\n",
    "    <tr><th>Name</th><th>Data Type</th><th>Metric</th><th>Description</th></tr>\n",
    "    <tr><td>Sample ID</td><td>integer</td><td></td><td>A unique number for each sample taken</td></tr>\n",
    "    <tr><td>Sex</td><td>nominal</td><td></td><td>M = 0, F = 1, and I = 2 (infant)</td></tr>\n",
    "\t<tr><td>Length</td><td>continuous</td><td>mm</td><td>Longest shell measurement</td></tr>\n",
    "\t<tr><td>Diameter</td><td>continuous</td><td>mm</td><td>perpendicular to length</td></tr>\n",
    "\t<tr><td>Height</td><td>continuous</td><td>mm</td><td>with meat in shell</td></tr>\n",
    "\t<tr><td>Whole weight</td><td>continuous</td><td>grams</td><td>whole abalone</td></tr>\n",
    "\t<tr><td>Shucked weight</td><td>continuous</td><td>grams</td><td>weight of meat</td></tr>\n",
    "\t<tr><td>Viscera weight</td><td>continuous</td><td>grams</td><td>gut weight (after bleeding)</td></tr>\n",
    "\t<tr><td>Shell weight</td><td>continuous</td><td>grams</td><td>after being dried</td></tr>\n",
    "\t<tr><td>Rings</td><td>integer</td><td></td><td>+1.5 gives the age in years</td></tr>\n",
    "</table>\n",
    "\n",
    "***Note:*** To demonstrate specific techniques of data wrangling, the dataset provided to you was altered: a sample ID column was added, the Sex column contains numeric IDs, and missing values were added as were duplicates.\n",
    "\n",
    "This data has no header information, so, we'll provide it when we import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Data-Analytics-With-Python/data/abalone.data' does not exist: b'Data-Analytics-With-Python/data/abalone.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dfaa3913a78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabalone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data-Analytics-With-Python/data/abalone.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m abalone.columns = ['Sample_ID','Sex', 'Length', 'Diameter', 'Height', \n\u001b[1;32m      3\u001b[0m           \u001b[0;34m'Whole_weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Shucked_weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Viscera_weight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           'Shell_weight', 'Rings']\n\u001b[1;32m      5\u001b[0m \u001b[0mabalone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data-Analytics-With-Python/data/abalone.data' does not exist: b'Data-Analytics-With-Python/data/abalone.data'"
     ]
    }
   ],
   "source": [
    "abalone = pd.read_csv('afs505_u2/git files/data/abalone.data', header = None)\n",
    "abalone.columns = ['Sample_ID','Sex', 'Length', 'Diameter', 'Height', \n",
    "          'Whole_weight', 'Shucked_weight', 'Viscera_weight', \n",
    "          'Shell_weight', 'Rings']\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploring Data Types\n",
    "First, let's explore how Pandas imported the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abalone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-23a0906e7301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabalone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abalone' is not defined"
     ]
    }
   ],
   "source": [
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the first, second and last columns, all others were imported as `float64` which is a decimal value. The others were imported as an `integer`.  This looks correct for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of how big the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the distribution of numerical data using the `describe` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that even though the 'Sex' column was provided as a numeric value, it is actually meant to be categorical, with each sex represented as a unique number.  We can explore the categorical data using the `groupby` function, followed by the `size` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.groupby(by=['Sex']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Finding Missing Values\n",
    "Before proceeding with any analysis you should know the state of missing values in the dataset.  For most analytics missing values are not supported. Some tools will automatically ignore them but it may be easier, in some cases, to remove them.\n",
    "\n",
    "First, let's quantify how many missing values we have. The `isna` function will convert the data into `True` or `False` values: `True` if the value is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.isna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `sum` function to then identify how many missing values we have per column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inspecting Duplicates\n",
    "Sometimes we may or may not want duplicates in the data. This depends on the expectations of the experiments and the measurements taken. Sometimes duplicates may represent human error in data entry. So, let's look for duplicated data.  We have 4,184 rows, let's see how many unique values per column that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of the columns we have fewer that 4,184 values.  For columns like 'Sex' we have 3 unique values, but these repeated values are expected.  The decimal values also have duplicates. The likelihood of seeing the exact same decimal values varies based on the distribution for the variable and the number of decimal values in the measurement.  The number of duplicated values does not seem unordinary.  However, the sample ID should be unique, yet we have 4,177 of them instead of 4,184. This implies we have duplicated samples in the data. \n",
    "\n",
    "We can identify then umber of duplicated 'Sample_ID' values are in the data by using the `duplicated` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.duplicated(subset='Sample_ID').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7 duplicated rows. Now let's see which rows have duplicated samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone[abalone.duplicated(subset='Sample_ID', keep= False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the rows are exact duplicates, so this was probably human entry error. We need to remove the copies rows. We will do so in the **3.1 Filtering** section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cleanup\n",
    "### 3.1 Correcting Data Types\n",
    "During the data exploration phase above, we noticed that the Sex column was provided as a number to represent the Sex category, and therefore, Pandas imported that column as a numeric value. We need to convert that to a categorical value, because the meaning of the column is not ordinal or numeric. We should covert it to a string object.\n",
    "\n",
    "We can do that with two functions that work on Series:  \n",
    "- `astype`  converts the type of data in the series. \n",
    "- `replace`  replaces values in the series.\n",
    "\n",
    "We'll use `astype` to convert the column to a string and `replace` to convert the numbers to more easily recognizable 'Male', 'Female' and 'Infant' strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert the column from an integer to a string.\n",
    "sex = abalone['Sex'].astype(str)\n",
    "\n",
    "# Second convert 0 to Male, 1 to Female, and 2 to Infant.\n",
    "sex = sex.replace('0', 'Male')\n",
    "sex = sex.replace('1', 'Female')\n",
    "sex = sex.replace('2', 'Infant')\n",
    "\n",
    "# Now replace the 'Sex' column of the dataframe with the new Series.\n",
    "abalone['Sex'] = sex\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the Sample ID column, despite that it is numeric should not be treated as a numeric column, so let's convert that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sample_ID to a string\n",
    "abalone['Sample_ID'] = abalone['Sample_ID'].astype(str)\n",
    "\n",
    "# Let's check out the datatypes to make sure they match our expectations:\n",
    "abalone.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handling Missing Values\n",
    "As observed in section 2.2, we do indeed have missing values! Let's remove rows with missing values.  We can do so with the `dropna` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.dropna(axis=0)\n",
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the `axis` argument is set to 0 indicating we will remove rows with missing values. If we compare the `shape` of the dataframe now, with the shape when we first loaded it we will see that we have lost 2 rows with missing values.\n",
    "\n",
    "In addition to `dropna` you can also use the `fillna` and `replace` functions to rewrite the missing values to something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicates we can use the [drop_duplicates](http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.drop_duplicates.html) function of Pandas.  If we explore the duplicated columns of section 2.3 above we'll see that the rows are the same for all columns.  In this case we can call `drop_duplicates` with no arguments.  However, let's assume we can't guarantee that each column is the same,  but we do want to remove duplicated samples.  We can do this by using the `subset` argument of the `drop_duplicates` function. We don't want to drop all duplicates, we need to keep one set. Therefore, we'll use the `keep` argument to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop_duplicates(['Sample_ID'], keep='first')\n",
    "abalone.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the `keep` argument will default to `first` so we don't need to provide it, but including it makes the code more clear.  We have now dropped all duplicated rows and we have 4,177 valid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Reshaping Data\n",
    "Data reshaping is about altering the way data is housed in the data frames of Pandas. It includes filtering of rows, merging data frames, concatenating data frames, grouping, melting and pivoting. We have learned about all of these functions already. As a reminder, the following is a summary of what we've learned:\n",
    "\n",
    "**Subsetting by Column**:\n",
    "- *Indexing with column names*\n",
    "  - Purpose: Allows you to slice the dataframe using column index names.\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example:\n",
    "  ```python\n",
    "   # Get the columns: Sample_ID, Sex, Height and Rings\n",
    "   subset = abalone[['Sample_ID', 'Sex', 'Height', 'Rings']]\n",
    "  ```\n",
    "- *Indexing with the `loc` function*\n",
    "  - Purpose: Allows you to slice the dataframe using row and column index names.\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example:\n",
    "  ```python\n",
    "   # Get the columns: Sample_ID, Sex, Height and Rings\n",
    "   subset = abalone.loc[:,['Sample_ID', 'Sex', 'Height', 'Rings']]\n",
    "  ```    \n",
    "  \n",
    "**Filtering Rows**:\n",
    "- *Boolean Indexing*\n",
    "  - Purpose: to filter rows that match desired criteria\n",
    "  - Introduced:  Pandas Part 1 Notebook\n",
    "  - Example:  \n",
    "  ```python\n",
    "   # Finds all rows with sex of \"Male\" and the number of rings > 3.\n",
    "   matches = (abalone['Sex'] == 'Male') & (abalone['Rings'] > 3)\n",
    "   male = abalone[matches]\n",
    "\n",
    "   # Or more succinctly\n",
    "   male = abalone[(abalone['Sex'] == 'Male') & (abalone['Rings'] > 3)]\n",
    "  ```\n",
    "\n",
    "**Grouping Data**:\n",
    "- *`groupby` function*\n",
    "  - Purpose:  To group rows together by \"classes\" or values of data. Allows you to perform aggregate functions, such as calculating means, summations, sizes, etc. You can create new data frames with aggregated values.\n",
    "  - Introduced:  Pandas Part 2 Notebook. \n",
    "  - Example:\n",
    "  ```python\n",
    "  # Calculate the mean column value by each sex:\n",
    "  abalone.groupby(by=\"Sex\").mean()\n",
    "  ```\n",
    "  \n",
    "**Merging DataFrames**:\n",
    "- *`concat` function*\n",
    "  - Purpose: To combine two dataframes.  Depending if the columns and row indexes are the same determines how the data frames are combined.\n",
    "  - Introduced:  Pandas Part 2 Notebook.\n",
    "\n",
    "**Melting**:\n",
    "- *`melt` function*\n",
    "  - Purpose:  Handles the case where categorical observations are stored in the header labels (i.e. violates Tidy rules).  It moves the header names into a new column and matches the corresponding values.\n",
    "  - Introduced:  Tidy Part 1 Notebook.\n",
    "\n",
    "**Pivoting**:\n",
    "- *`pivot` and `pivot_table` functions*\n",
    "  - Purpose: The opposite of `melt`. Uses unique values from one more columns to create new columns.\n",
    "  - Intorduced: Tidy Part 1 Notebook.\n",
    "  \n",
    "You can use any of these functions/techniques to reshape the data to meet Tidy standards and appropriate for the analytic or visualization you want to perform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
